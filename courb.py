# -*- coding: utf-8 -*-
"""CoUrb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CIJcIEHc7a4VkldxlFV9OAhKhBIsTB3u
"""

!pip install --upgrade --quiet opencv-python numpy tensorflow sklearn keras

import numpy as np
import random
import cv2
import os
from sklearn.metrics import accuracy_score
import array
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Activation
from tensorflow.keras.layers import Dense

def load(paths, verbose=-1):
    data = list()
    labels = list()
    for (i, imgpath) in enumerate(paths):
        im_gray = cv2.imread(imgpath, cv2.IMREAD_GRAYSCALE)
        image = np.array(im_gray).flatten()
        label = imgpath.split(os.path.sep)[-2]

        data.append(image / 255)
        labels.append(label)

        if verbose > 0 and i > 0 and (i + 1) % verbose == 0:
            print("[INFO] processed {}/{}".format(i + 1, len(paths)))

    return data, labels


def create_clients(image_list, label_list, num_clients=10, initial='clients'):

    client_names = ['{}_{}'.format(initial, i + 1) for i in range(num_clients)]

    data = list(zip(image_list, label_list))
    random.shuffle(data)

    size = len(data) // num_clients
    shards = [data[i:i + size] for i in range(0, size * num_clients, size)]

    assert (len(shards) == len(client_names))
    return {client_names[i]: shards[i] for i in range(len(client_names))}

def batch_data(data_shard, bs=32):
    data, label = zip(*data_shard)
    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))
    return dataset.shuffle(len(label)).batch(bs)

class SimpleMLP:
    @staticmethod
    def build(shape, classes):
        model = Sequential()
        model.add(Dense(200, input_shape=(shape,)))
        model.add(Activation("relu"))
        model.add(Dense(200))
        model.add(Activation("relu"))
        model.add(Dense(classes))
        model.add(Activation("softmax"))
        return model

def weight_scalling_factor(clients_trn_data, client_name):
    client_names = list(clients_trn_data.keys())
    bs = list(clients_trn_data[client_name])[0][0].shape[0]

    global_count = sum(
        [tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names]) * bs

    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() * bs
    return local_count / global_count

def scale_model_weights(weight, scalar):
    weight_final = []
    steps = len(weight)
    for i in range(steps):
        weight_final.append(scalar * weight[i])
    return weight_final

def sum_scaled_weights(scaled_weight_list):
    avg_grad = list()
    for grad_list_tuple in zip(*scaled_weight_list):
        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)
        avg_grad.append(layer_mean)

    return avg_grad

def test_model(X_test, Y_test, model, comm_round):
    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)
    logits = model.predict(X_test)
    loss = cce(Y_test, logits)
    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))

    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))

    return acc, loss

from google.colab import files
uploaded = files.upload()

!ls

!tar xvf trainingSet.tar.gz

from imutils import paths
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split

from tensorflow.keras.optimizers import SGD
from tensorflow.keras import backend as K

import matplotlib.pyplot as plt

#from utils import *



img_path = 'trainingSet/'

image_paths = list(paths.list_images(img_path))

image_list, label_list = load(image_paths, verbose=10000)

lb = LabelBinarizer()
label_list = lb.fit_transform(label_list)

X_train, X_test, y_train, y_test = train_test_split(image_list,
                                                    label_list,
                                                    test_size=0.1,
                                                    random_state=42)
clients = create_clients(X_train, y_train, num_clients=10, initial='client')

clients_batched = dict()
for (client_name, data) in clients.items():
    clients_batched[client_name] = batch_data(data)

test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))

comms_round = 2

lr = 0.01
loss = 'categorical_crossentropy'
metrics = ['accuracy']
optimizer = SGD(lr=lr,
                decay=lr / comms_round,
                momentum=0.9
                )
smlp_global = SimpleMLP()
global_model = smlp_global.build(784, 10)

for comm_round in range(comms_round):

    global_weights = global_model.get_weights()

    scaled_local_weight_list = list()
    client_names = list(clients_batched.keys())
    random.shuffle(client_names)

    for client in client_names:
        smlp_local = SimpleMLP()
        local_model = smlp_local.build(784, 10)
        local_model.compile(loss=loss,
                            optimizer=optimizer,
                            metrics=metrics)

        local_model.set_weights(global_weights)

        local_model.fit(clients_batched[client], epochs=1, verbose=0)

        scaling_factor = weight_scalling_factor(clients_batched, client)
        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)
        scaled_local_weight_list.append(scaled_weights)

        K.clear_session()

    average_weights = sum_scaled_weights(scaled_local_weight_list)

    global_model.set_weights(average_weights)

    accs  = []
    lost  = []
    for (X_test, Y_test) in test_batched:
        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)
        accs  = np.append(accs,global_acc)
        lost  = np.append(lost,global_loss)
        SGD_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(y_train)).batch(320)


smlp_SGD = SimpleMLP()
SGD_model = smlp_SGD.build(784, 10)

SGD_model.compile(loss=loss,
                  optimizer=optimizer,
                  metrics=metrics)

fit = SGD_model.fit(SGD_dataset, epochs=200, verbose=0)

plt.plot(fit.history['loss'],label='loss')
plt.plot(fit.history['accuracy'],label='acc')
plt.xlabel('Epochs')
plt.ylabel('Percentage')
plt.legend()
plt.show()

for (X_test, Y_test) in test_batched:
  SGD_acc, SGD_loss = test_model(X_test, Y_test, SGD_model, 1)